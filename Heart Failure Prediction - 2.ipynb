{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0fd61926-d8e2-4547-8f40-bcc552b67041",
   "metadata": {},
   "source": [
    "# Heart Disease Prediction Using Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70dc2d56-62d7-41c5-9e26-26f0ca3e2b68",
   "metadata": {},
   "source": [
    "# Abstract"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a082ae-0af9-45b7-a032-16cd780a8ca5",
   "metadata": {},
   "source": [
    "### Cardiovascular diseases (CVDs) remain the leading cause of mortality worldwide, responsible for nearly 17.9 million deaths annually. Among CVDs, heart disease and heart failure are major contributors. Early detection and intervention are critical in reducing fatalities. However, traditional diagnostic methods such as electrocardiograms (ECG), echocardiography, stress tests, and blood biomarkers often require specialized equipment, expert evaluation, and high costs, making them inaccessible to many populations. With the rise of machine learning (ML) in healthcare, automated heart disease prediction models offer efficient, cost-effective, and scalable solutions.\n",
    "\n",
    "### This study investigates the application of four ML models—K-Nearest Neighbors (KNN), Logistic Regression, Decision Tree, and Random Forest classifiers—to predict heart disease using the Heart Failure Prediction Dataset from Kaggle (Fedesoriano, 2021). The dataset consists of 918 patient records with 12 clinical features, including age, cholesterol levels, blood pressure, heart rate, and chest pain type. The data was preprocessed through missing value imputation, categorical encoding, and feature scaling to improve model performance.\n",
    "\n",
    "#### Model performance was evaluated using accuracy, precision, recall, F1-score, and confusion matrices. Among the models:\n",
    "\n",
    "##### Random Forest achieved the highest accuracy (94%), demonstrating its robustness and ability to handle complex data relationships.\n",
    "##### Logistic Regression performed well (93%), making it an interpretable and clinically applicable model.\n",
    "##### Decision Tree achieved 83% accuracy, but suffered from overfitting.\n",
    "##### KNN had the lowest accuracy (74%), struggling with high-dimensional data.\n",
    "##### The confusion matrix analysis showed that Random Forest had the lowest misclassification rate, making it the best candidate for heart disease prediction.\n",
    "\n",
    "### The study concludes that machine learning can serve as a valuable decision-support tool in cardiology, aiding early diagnosis and risk stratification. Future research should explore deep learning techniques, feature engineering, and real-time patient monitoring systems to improve accuracy and usability in clinical settings. Integration with electronic health records (EHRs) and wearable technology can further enhance early detection, reduce misdiagnosis, and improve patient outcomes.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c12ecc43-78c2-4689-b2e8-6832bf39be10",
   "metadata": {},
   "source": [
    "# 1. Introduction "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c4d7ee1-25b2-435e-aae9-4449a82648e5",
   "metadata": {},
   "source": [
    "## 1.1 Heart Disease and Heart Failure\n",
    "### Heart disease refers to a range of cardiovascular conditions affecting the heart’s structure and function, including coronary artery disease (CAD), arrhythmias, heart failure, and valvular disorders. Among these, CAD is the most prevalent, caused by the buildup of plaque in the arteries, leading to reduced blood supply and increasing the risk of heart attacks and strokes.\n",
    "\n",
    "### Heart failure, on the other hand, occurs when the heart cannot pump blood efficiently, resulting in fluid accumulation in the lungs and other tissues. This condition can be chronic or acute, and it significantly reduces a patient's quality of life and life expectancy. Common symptoms include shortness of breath, fatigue, swelling in the legs, and persistent coughing.\n",
    "\n",
    "## 1.2 Current Diagnostic Technologies\n",
    "#### Modern medicine relies on a variety of diagnostic tools to detect heart disease, including:\n",
    "\n",
    "####  Electrocardiograms (ECG): Records electrical activity to detect arrhythmias and abnormal heart function.\n",
    "#### Echocardiography: Uses ultrasound to examine heart structure and blood flow.\n",
    "#### Cardiac MRI and CT Scans: Provide detailed imaging of heart tissues and arteries.\n",
    "#### Blood Tests: Measure biomarkers like troponin, cholesterol, and glucose levels.\n",
    "#### While these techniques are effective, they have limitations such as high costs, accessibility issues, and reliance on expert interpretation. This creates a need for automated, scalable solutions that can assist in early disease detection.\n",
    "\n",
    "## 1.3 Machine Learning in Heart Disease Prediction\n",
    "### Machine learning (ML) has emerged as a transformative technology in healthcare, offering data-driven predictive models that can analyze vast patient records and detect disease patterns. ML can enhance early diagnosis, reduce misdiagnosis, and optimize treatment decisions.\n",
    "\n",
    "### This study applies four ML algorithms—KNN, Logistic Regression, Decision Tree, and Random Forest—to predict heart disease based on clinical attributes. The goal is to evaluate their effectiveness and potential for real-world application."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ebc205d-5bd8-4619-a859-cdc401a83754",
   "metadata": {},
   "source": [
    "# 2. Methodology"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0dcc071-94db-48c3-9d6f-ca9d23fb5941",
   "metadata": {},
   "source": [
    "## Data Collection and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "952eea5d-052d-4401-9b4a-4fdf5891ee96",
   "metadata": {},
   "source": [
    "## The dataset was sourced from Kaggle (Fedesoriano, Heart Failure Prediction Dataset), containing 918 patient records with 12 features, including:\n",
    "\n",
    "#### Demographic Data: Age, Sex\n",
    "#### Clinical Parameters: Blood pressure, Cholesterol, Fasting Blood Sugar\n",
    "#### Exercise-Related Factors: Maximum Heart Rate, Exercise-Induced Angina\n",
    "#### ECG Readings: Resting ECG, ST Slope\n",
    "#### Target Variable: Presence of Heart Disease (1 = Yes, 0 = No)\n",
    "## Data Preprocessing Steps\n",
    "#### Missing Values: Handled using KNN Imputation.\n",
    "#### Categorical Encoding: Converted categorical data using One-Hot Encoding.\n",
    "#### Feature Scaling: Applied MinMaxScaler to normalize numerical values.\n",
    "#### Train-Test Split: Divided into 80% training and 20% testing data."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3f616c06-649d-4e62-9460-46a76ea8f5f1",
   "metadata": {},
   "source": [
    "## Libraries and Tools Used\n",
    "### To implement the machine learning models, the following Python libraries were used:\n",
    "\n",
    "#### Pandas – For data manipulation and preprocessing\n",
    "#### NumPy – For numerical operations\n",
    "#### Matplotlib & Seaborn – For data visualization (histograms, heatmaps, and correlation plots)\n",
    "#### Scikit-learn (sklearn) – For machine learning model implementation, including:\n",
    "#### train_test_split (to split the dataset)\n",
    "#### StandardScaler & MinMaxScaler (for feature scaling)\n",
    "#### KNeighborsClassifier (KNN model)\n",
    "#### LogisticRegression (Logistic Regression model)\n",
    "#### DecisionTreeClassifier (Decision Tree model)\n",
    "#### RandomForestClassifier (Random Forest model)\n",
    "#### classification_report (for model performance evaluation)\n",
    "#### confusion_matrix (to assess misclassifications)\n",
    "#### MinMaxScaler – Normalizes numerical features to improve model accuracy and prevent scale bias.\n",
    "#### LabelEncoder – Converts categorical variables (e.g., chest pain type, sex) into a numerical format suitable for machine learning models.\n",
    "#### KNNImputer – Handles missing values using the K-Nearest Neighbors approach."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "37519980-f12c-4fd0-bb08-027b90ef4946",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt \n",
    "import seaborn as sns\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "f7a96106-7739-4938-a992-495bca9ee313",
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'Downloads/heart failure detection/heart.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m df \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDownloads/heart failure detection/heart.csv\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1026\u001b[0m, in \u001b[0;36mread_csv\u001b[1;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[0;32m   1013\u001b[0m kwds_defaults \u001b[38;5;241m=\u001b[39m _refine_defaults_read(\n\u001b[0;32m   1014\u001b[0m     dialect,\n\u001b[0;32m   1015\u001b[0m     delimiter,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1022\u001b[0m     dtype_backend\u001b[38;5;241m=\u001b[39mdtype_backend,\n\u001b[0;32m   1023\u001b[0m )\n\u001b[0;32m   1024\u001b[0m kwds\u001b[38;5;241m.\u001b[39mupdate(kwds_defaults)\n\u001b[1;32m-> 1026\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m _read(filepath_or_buffer, kwds)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:620\u001b[0m, in \u001b[0;36m_read\u001b[1;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[0;32m    617\u001b[0m _validate_names(kwds\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mnames\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m))\n\u001b[0;32m    619\u001b[0m \u001b[38;5;66;03m# Create the parser.\u001b[39;00m\n\u001b[1;32m--> 620\u001b[0m parser \u001b[38;5;241m=\u001b[39m TextFileReader(filepath_or_buffer, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwds)\n\u001b[0;32m    622\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m chunksize \u001b[38;5;129;01mor\u001b[39;00m iterator:\n\u001b[0;32m    623\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m parser\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1620\u001b[0m, in \u001b[0;36mTextFileReader.__init__\u001b[1;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[0;32m   1617\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m kwds[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhas_index_names\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[0;32m   1619\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles: IOHandles \u001b[38;5;241m|\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m-> 1620\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_engine \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_make_engine(f, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mengine)\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py:1880\u001b[0m, in \u001b[0;36mTextFileReader._make_engine\u001b[1;34m(self, f, engine)\u001b[0m\n\u001b[0;32m   1878\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m mode:\n\u001b[0;32m   1879\u001b[0m         mode \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m-> 1880\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;241m=\u001b[39m get_handle(\n\u001b[0;32m   1881\u001b[0m     f,\n\u001b[0;32m   1882\u001b[0m     mode,\n\u001b[0;32m   1883\u001b[0m     encoding\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1884\u001b[0m     compression\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompression\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1885\u001b[0m     memory_map\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmemory_map\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mFalse\u001b[39;00m),\n\u001b[0;32m   1886\u001b[0m     is_text\u001b[38;5;241m=\u001b[39mis_text,\n\u001b[0;32m   1887\u001b[0m     errors\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mencoding_errors\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstrict\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[0;32m   1888\u001b[0m     storage_options\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptions\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mstorage_options\u001b[39m\u001b[38;5;124m\"\u001b[39m, \u001b[38;5;28;01mNone\u001b[39;00m),\n\u001b[0;32m   1889\u001b[0m )\n\u001b[0;32m   1890\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m   1891\u001b[0m f \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mhandles\u001b[38;5;241m.\u001b[39mhandle\n",
      "File \u001b[1;32m~\\anaconda3\\Lib\\site-packages\\pandas\\io\\common.py:873\u001b[0m, in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[0;32m    868\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(handle, \u001b[38;5;28mstr\u001b[39m):\n\u001b[0;32m    869\u001b[0m     \u001b[38;5;66;03m# Check whether the filename is to be opened in binary mode.\u001b[39;00m\n\u001b[0;32m    870\u001b[0m     \u001b[38;5;66;03m# Binary mode does not support 'encoding' and 'newline'.\u001b[39;00m\n\u001b[0;32m    871\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mencoding \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mb\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m ioargs\u001b[38;5;241m.\u001b[39mmode:\n\u001b[0;32m    872\u001b[0m         \u001b[38;5;66;03m# Encoding\u001b[39;00m\n\u001b[1;32m--> 873\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(\n\u001b[0;32m    874\u001b[0m             handle,\n\u001b[0;32m    875\u001b[0m             ioargs\u001b[38;5;241m.\u001b[39mmode,\n\u001b[0;32m    876\u001b[0m             encoding\u001b[38;5;241m=\u001b[39mioargs\u001b[38;5;241m.\u001b[39mencoding,\n\u001b[0;32m    877\u001b[0m             errors\u001b[38;5;241m=\u001b[39merrors,\n\u001b[0;32m    878\u001b[0m             newline\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[0;32m    879\u001b[0m         )\n\u001b[0;32m    880\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    881\u001b[0m         \u001b[38;5;66;03m# Binary mode\u001b[39;00m\n\u001b[0;32m    882\u001b[0m         handle \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mopen\u001b[39m(handle, ioargs\u001b[38;5;241m.\u001b[39mmode)\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'Downloads/heart failure detection/heart.csv'"
     ]
    }
   ],
   "source": [
    "df = pd.read_csv('Downloads/heart failure detection/heart.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddbd666-756f-47fc-bda5-8798b92a0200",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6187ce36-63c5-47b3-b624-6640c8daa35d",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc02e0b-71ff-4f45-8ade-c9cca3da7e95",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49249409-3557-4c88-8173-7507379ab27d",
   "metadata": {},
   "source": [
    "## 2.1 Exploratory Data Analysis (EDA)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f870dd2a-e67b-429c-a53f-00c8b48d2e7b",
   "metadata": {},
   "source": [
    "### EDA was performed to understand data patterns and relationships:\n",
    "\n",
    "#### Age Distribution Analysis: Most heart disease cases occurred in individuals aged 50 and above.\n",
    "####  Cholesterol Levels: Higher cholesterol levels were strongly correlated with heart disease presence.\n",
    "#### Pain Type Analysis: Asymptomatic chest pain (ASY) was the most common indicator of heart disease.\n",
    "#### Feature Correlation Analysis: A heatmap showed strong correlations between cholesterol, age, and ST slope with heart disease."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9de9878b-305c-45af-b828-b55fbe80860f",
   "metadata": {},
   "source": [
    "#### Visualizations:\n",
    "\n",
    "##### Bar plots to analyze the distribution of categorical variables.\n",
    "##### Histograms for age and cholesterol distribution.\n",
    "##### A heatmap to observe correlations among features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b5865f9-1189-4bfd-93b1-11e51edbb887",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3894fc2-687f-4516-b5cd-e003f622ee16",
   "metadata": {},
   "source": [
    "## The average age of the patients is around 54 years old, indicating that the dataset may be biased towards middle-aged individuals."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fa432f73-6b2e-4c22-bdb1-4f06228010c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb33c23f-410b-4cba-bd60-c6b608856483",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.duplicated().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "216cd389-f85b-4032-9a9f-3096c21eadc1",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['RestingBP'].replace(0, np.nan, inplace=True)\n",
    "df['Cholesterol'].replace(0, np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f390a3bb-bd25-4223-bd60-ab128741f29c",
   "metadata": {},
   "outputs": [],
   "source": [
    "df.isna().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0dbc8df-aa50-4272-825a-7cf6b17bea01",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[['RestingBP', 'Cholesterol']].isna().mean() * 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a39bed-cf90-456b-8429-d1c9131d8ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df[df[\"Cholesterol\"].isna()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3c918e7-48f5-4a9a-a694-659797a2e9df",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(df.columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00dfc94c-e5f2-493d-95f8-d8580c591ab0",
   "metadata": {},
   "source": [
    "## For each categorical column, including HeartDisease and FastingBS, you can create a bar chart to display the number of rows for each category.\n",
    "\n",
    "## First, you need to identify the categorical columns. Assuming df is your DataFrame, you can select categorical columns by their data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "887b1171-7cb4-4474-8724-09bcf0a501c8",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3301514d-7c87-40aa-b814-83b9eb2dfd5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_cols:\n",
    "    plt.figure(figsize=(10,6))\n",
    "    df[col].value_counts().plot(kind='bar')\n",
    "    plt.title(f'Distribution of {col}')\n",
    "    plt.xlabel('Category')\n",
    "    plt.ylabel('Count')\n",
    "    \n",
    "    # Add data labels\n",
    "    for p in plt.gca().patches:\n",
    "        plt.gca().text(p.get_x() + p.get_width()/2, p.get_height(), str(p.get_height()), ha='center', va='bottom')\n",
    "    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12ee2734-dc69-4e02-b001-5ac6ef97d7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de9e1c07-cc6d-433e-abe2-6f2615413bc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "for col in categorical_cols:\n",
    "    if col != 'HeartDisease':\n",
    "        plt.figure(figsize=(10,6))\n",
    "        pivot_table = pd.pivot_table(df, index=col, columns='HeartDisease', aggfunc='size', fill_value=0)\n",
    "        pivot_table.plot(kind='bar')\n",
    "        plt.title(f'Distribution of {col} by HeartDisease')\n",
    "        plt.xlabel('Category')\n",
    "        plt.ylabel('Count')\n",
    "        plt.legend(title='HeartDisease')\n",
    "        \n",
    "        # Add data labels\n",
    "        for p in plt.gca().patches:\n",
    "            width, height = p.get_width(), p.get_height()\n",
    "            x, y = p.get_xy() \n",
    "            plt.gca().text(x + width/2, y + height/2, str(int(height)), horizontalalignment='center', verticalalignment='center')\n",
    "        \n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9955600b-5850-4d0a-94b1-06c98cd4aa08",
   "metadata": {},
   "source": [
    "## ChestPainType has a higher count for patients with heart disease, and any other interesting patterns you observe in the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9654c683-7c17-49b1-8938-926227cfe1cb",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_restingbp_count = (df['RestingBP'] == 0).sum()\n",
    "print(f'Number of rows with 0 value for RestingBP: {zero_restingbp_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "723f84df-92cb-457d-b4d5-d8a5dddab927",
   "metadata": {},
   "outputs": [],
   "source": [
    "zero_cholesterol_count = len(df[df['Cholesterol'] == 0])\n",
    "print(f'Number of rows with 0 value for Cholesterol: {zero_cholesterol_count}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844b9fed-56bd-44f4-856c-e7349bfba781",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "df = pd.DataFrame(df)\n",
    "\n",
    "# Calculate median values\n",
    "median_RestingBP = df['RestingBP'].median()\n",
    "median_Cholesterol = df['Cholesterol'].median()\n",
    "\n",
    "print(\"Median RestingBP:\", median_RestingBP)\n",
    "print(\"Median Cholesterol:\", median_Cholesterol)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a587814-7e8c-4f99-9d59-21b9300c3d9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "categorical_cols = df.select_dtypes(include=['object', 'category']).columns\n",
    "dummy_df = pd.get_dummies(df, columns=categorical_cols)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a8c6c58b-a7ce-4611-b24b-25081738f8d6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.get_dummies(df, drop_first=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "38a5fd3e-02fb-41db-8b0c-9b280188894a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculate the correlation matrix\n",
    "correlation_matrix = df.corr()\n",
    "\n",
    "# Create a heat map\n",
    "plt.figure(figsize=(10,8))\n",
    "sns.heatmap(correlation_matrix, annot=True, cmap='coolwarm', square=True)\n",
    "plt.title('Pearson Correlation Heat Map')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf87b597-8cdc-4e95-9078-ef67745e4869",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('Downloads/heart failure detection/heart.csv')\n",
    "print(df.columns)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abe15211-5cc3-4eb5-95db-905edafe2cd6",
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e0cc38e-d43f-4003-a682-4d0f16f42357",
   "metadata": {},
   "outputs": [],
   "source": [
    "genders = df['Sex'].value_counts()\n",
    "genders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b8d9a3f-d569-469f-902f-6afa7139b551",
   "metadata": {},
   "outputs": [],
   "source": [
    "genders.plot(kind='bar', color=['Red', 'black'], edgecolor='black')\n",
    "plt.xlabel('Gender')\n",
    "plt.ylabel('Count')\n",
    "plt.title('Gender Distribution')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbaaa58e-7948-4cc7-b8c4-870f7ab28f2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "chest_pain = df['ChestPainType'].value_counts()\n",
    "chest_pain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74dc4385-3610-45d9-9f28-ccbace37cb5e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Plot a donut chart\n",
    "chest_pain.value_counts().plot(kind='pie', autopct='%1.1f%%', figsize=(6, 6), wedgeprops={'linewidth': 1, 'edgecolor': 'white'})\n",
    "\n",
    "# Add a white circle in the center to create the donut effect\n",
    "plt.gca().add_artist(plt.Circle((0, 0), 0.5, fc='Beige'))\n",
    "\n",
    "plt.ylabel('')\n",
    "plt.title('Chest Pain Distribution')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70a13621-fd3c-4f96-b4ba-47970b8383ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "df['RestingECG'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2c96fa99-9dd6-4253-87e2-3624dad0c641",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "sns.countplot(x=df['RestingECG'], palette='pastel', edgecolor='black')\n",
    "plt.xlabel('RestingECG Type')\n",
    "plt.ylabel('Count')\n",
    "plt.title('RestingECG Distribution')\n",
    "plt.xticks(rotation=0)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc8c5f8-a3cf-4d89-bc52-3e4ce345f20f",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.crosstab(df['Sex'], df['ChestPainType'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85281153-adf1-4b24-b7c6-db6fe6ebaaa2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import seaborn as sns\n",
    "\n",
    "sns.heatmap(pd.crosstab(df['Sex'], df['ChestPainType']), annot=True, cmap='viridis', fmt='d')\n",
    "plt.title('Heatmap of Chest Pain Type by Sex')\n",
    "plt.ylabel('Sex')\n",
    "plt.xlabel('Chest Pain Type')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba3047d7-29d4-4a82-883f-2a76ea1b33a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(8,6))\n",
    "sns.histplot(data=df, x='Age', hue='HeartDisease', kde=True, bins=20, palette='coolwarm', alpha=0.6)\n",
    "plt.title(\"Age Distribution by Heart Disease Status\")\n",
    "plt.xlabel(\"Age\")\n",
    "plt.ylabel(\"Count\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05fae9df-8ac2-4968-b462-209b45769e73",
   "metadata": {},
   "outputs": [],
   "source": [
    "corr = df.corr(numeric_only=True)\n",
    "sns.heatmap(corr, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "825d49ee-7507-41a0-8a0f-bbf96aa3a59d",
   "metadata": {},
   "source": [
    "# 3 Model Selection and Training "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91ac2970-73d5-4731-aaa0-9f967fd17a53",
   "metadata": {},
   "source": [
    "### To develop an effective heart disease prediction system, we implemented and evaluated four machine learning models: K-Nearest Neighbors (KNN), Logistic Regression, Decision Tree Classifier, and Random Forest Classifier. Each model was trained on 80% of the dataset and tested on 20% to assess generalization performance. The models were evaluated based on accuracy, precision, recall, and F1-score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6935db74-0395-4b4c-b9e6-8759279c7e55",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import MinMaxScaler, LabelEncoder\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "21dae3c9-e132-4633-ace0-0bd215fe5a33",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.impute import KNNImputer\n",
    "\n",
    "# Use KNN imputation\n",
    "imputer = KNNImputer(n_neighbors=5)\n",
    "df[['RestingBP', 'Cholesterol']] = imputer.fit_transform(df[['RestingBP', 'Cholesterol']])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eca5ae20-f7ad-41b6-8c6c-ed0ec155f6da",
   "metadata": {},
   "outputs": [],
   "source": [
    "features = list()\n",
    "for feature in df.select_dtypes(include=['number']):\n",
    "    Q1 = df[feature].quantile(0.25)\n",
    "    Q3 = df[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5*IQR\n",
    "    upper_bound = Q3 + 1.5*IQR\n",
    "    if df[(df[feature] < lower_bound) | (df[feature] > upper_bound)].any().any():\n",
    "        print('yes', feature)\n",
    "        features.append(feature)\n",
    "    else:\n",
    "        print('no')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecce841-4684-460e-9859-5dad666f9efd",
   "metadata": {},
   "outputs": [],
   "source": [
    "features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "821713bc-f0b8-484e-a029-d2eb96bf61dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "for feature in features:\n",
    "    Q1 = df[feature].quantile(0.25)\n",
    "    Q3 = df[feature].quantile(0.75)\n",
    "    IQR = Q3 - Q1\n",
    "    lower_bound = Q1 - 1.5*IQR\n",
    "    upper_bound = Q3 + 1.5*IQR\n",
    "    df[(df[feature] >= lower_bound) | (df[feature] <= upper_bound)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8a55e86-3002-404e-9701-8d0d722ef0b8",
   "metadata": {},
   "outputs": [],
   "source": [
    "le = LabelEncoder()\n",
    "for column in df.select_dtypes(include=['object']):\n",
    "    df[column] = le.fit_transform(df[column])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4d0a27b-1482-4fde-a13d-33600116a349",
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('HeartDisease', axis=1)\n",
    "y = df['HeartDisease']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc56795-0138-4876-a2dd-3484a2a6aea4",
   "metadata": {},
   "outputs": [],
   "source": [
    "scaler = MinMaxScaler()\n",
    "scaler.fit_transform(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a577a73-30c0-41b1-b984-1a90a237bd29",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, train_size=0.8, stratify=y, random_state=42)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60f67ae7-bc27-428f-9400-664183e5fff0",
   "metadata": {},
   "source": [
    "## 3.1 K-Nearest Neighbors (KNN) – 74% Accuracy\n",
    "### The KNN algorithm is a non-parametric, instance-based learning method that classifies data points based on the majority class among their k-nearest neighbors. KNN is often used for pattern recognition but struggles with high-dimensional data.\n",
    "\n",
    "### Performance: Achieved 74% accuracy, which was the lowest among all models.\n",
    "#### Limitations:\n",
    "#### KNN requires distance-based calculations, which become inefficient with large feature spaces.\n",
    "#### It is sensitive to noise and imbalanced datasets, leading to misclassifications.\n",
    "#### It performed poorly in predicting heart disease due to overlapping feature distributions.\n",
    "#### Conclusion: KNN is not the ideal choice for this dataset due to its high computational cost and lower predictive accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f036d32-22cb-4f65-88c0-1251ca92d0f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "knn = KNeighborsClassifier(n_neighbors=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "01923034-9fba-489a-990c-0302dae93a37",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fit the model to the training data on the scaled features\n",
    "knn.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "624995bd-97ca-4e76-b9e8-1994e86ea5a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(X_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9af308d5-689f-4b6b-bbb8-cf5e7832bb28",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score\n",
    "accuracy = accuracy_score(y_test, y_pred)\n",
    "print(f\"Accuracy: {accuracy:.2f}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39170c11-970b-4b68-8f4d-926885360dc2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "016e6897-598a-438a-a975-72f64dff9f0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted Label\")\n",
    "plt.ylabel(\"True Label\")\n",
    "plt.title(\"Confusion Matrix\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4ca93753-546a-4bb4-8240-2e815b6b7e13",
   "metadata": {},
   "source": [
    "## The K-Nearest Neighbors (KNN) model achieved an accuracy of **70%**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4e37d86-5b5e-48be-af7d-3e5601189a2d",
   "metadata": {},
   "source": [
    "# 3.2 Logistic Regression – 93% Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d60c6dba-f576-4101-9432-e7da30dbc3a0",
   "metadata": {},
   "source": [
    "## Logistic Regression is a widely used statistical model for binary classification problems, including medical diagnostics. It estimates the probability that a given instance belongs to a particular class using the logistic (sigmoid) function.\n",
    "\n",
    "### Performance: Achieved 93% accuracy, making it one of the top-performing models.\n",
    "### Advantages:\n",
    "#### Well-suited for datasets with linear decision boundaries.\n",
    "#### Interpretable model that provides probability estimates.\n",
    "#### Less prone to overfitting compared to decision trees.\n",
    "### Limitations:\n",
    "#### Assumes linear relationships between input variables, which may not always hold.\n",
    "#### May not capture complex interactions between features.\n",
    "### Conclusion: Logistic Regression performed well, making it a strong candidate for practical applications in heart disease prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "424d8946-1b46-4a65-b1f9-b905b76cda0e",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LogisticRegression()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5da642fa-aa81-4760-b4d6-6bbfa0bdbebf",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2902da3c-5c1e-4b6b-922c-7f5b1376b741",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_lr = lr.predict(X_test)\n",
    "y_pred_lr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529b488b-1ad6-48ad-a789-6eea48815a92",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_lr))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2d1ca9f-15b3-4242-ad00-84fa05520d04",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_lr)\n",
    "ax = sns.heatmap(cm, annot=True, fmt='d', cmap=\"Greens\")\n",
    "ax.xaxis.set_ticklabels(['Rejected', 'Approved']) \n",
    "ax.yaxis.set_ticklabels(['Rejected', 'Approved']) \n",
    "ax.set_xlabel(\"Predicted\")\n",
    "ax.set_ylabel(\"Target\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b52d4f1-b64a-42fe-a85b-8f9f80514f4c",
   "metadata": {},
   "source": [
    "# 3.3 Decision Tree Classifier – 83% Accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d1d8a79-ad57-47d6-85f0-554c4516edb5",
   "metadata": {},
   "source": [
    "## Decision Trees use hierarchical, rule-based learning to classify data. They split datasets recursively into decision nodes until a prediction is made.\n",
    "\n",
    "## Performance: Achieved 83% accuracy, performing better than KNN but lower than Logistic Regression and Random Forest.\n",
    "### Advantages:\n",
    "#### Handles non-linear relationships effectively.\n",
    "####  Works well with small datasets and provides human-readable decision paths.\n",
    "### Limitations:\n",
    "####  Highly prone to overfitting, leading to lower generalization.\n",
    "####  Sensitive to small variations in data, resulting in unstable performance.\n",
    "### Conclusion: Decision Trees are useful for exploratory analysis, but overfitting limits their real-world reliability."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3987ec9-20be-4989-8173-84d09570b4e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt = DecisionTreeClassifier()\n",
    "dt.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ffe5d5a-fd47-41f6-a46b-e80c9d363f15",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_dt = dt.predict(X_test)\n",
    "y_pred_dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a304904-0adc-481d-a7d6-1e7b2fe1779b",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_dt))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2f4519ab-bd21-4cd7-915d-6cc74b93ab36",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_dt)\n",
    "ax = sns.heatmap(cm, annot=True, fmt='d')\n",
    "ax.xaxis.set_ticklabels(['Rejected', 'Approved']) \n",
    "ax.yaxis.set_ticklabels(['Rejected', 'Approved']) \n",
    "ax.set_xlabel(\"Predicted\")\n",
    "ax.set_ylabel(\"Target\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42375043-24ff-450c-9c41-f079d77f29dc",
   "metadata": {},
   "source": [
    "# 3.4 Random Forest Classifier – 94% Accuracy\n",
    "### Random Forest is an ensemble learning model that combines multiple decision trees to improve classification accuracy and reduce overfitting. It works by aggregating predictions from different trees, leading to more robust and stable results.\n",
    "\n",
    "## Performance: Achieved 94% accuracy, making it the best-performing model.\n",
    "### Advantages:\n",
    "#### Reduces overfitting by averaging multiple decision trees.\n",
    "#### Handles high-dimensional datasets efficiently.\n",
    "#### Provides feature importance rankings, helping in model interpretability.\n",
    "### Limitations:\n",
    "#### Requires higher computational power than single decision trees.\n",
    "#### Less interpretable compared to Logistic Regression and Decision Trees.\n",
    "### Conclusion: Due to its high accuracy and robustness, Random Forest was chosen as the optimal model for heart disease prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50f3798a-81aa-4931-ab21-3b094b73ddd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "rf =  RandomForestClassifier()\n",
    "rf.fit(X_train, y_train) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d85469d-a634-47dc-a83a-b939c4f4ee30",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_rf = rf.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e93a608-9ba6-4977-b510-79d2ec785d8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred_rf))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cd1f1f17-3dda-4890-9006-154748cf07d4",
   "metadata": {},
   "outputs": [],
   "source": [
    "cm = confusion_matrix(y_test, y_pred_rf)\n",
    "ax = sns.heatmap(cm, annot=True, fmt='d')\n",
    "ax.xaxis.set_ticklabels(['Rejected', 'Approved']) \n",
    "ax.yaxis.set_ticklabels(['Rejected', 'Approved']) \n",
    "ax.set_xlabel(\"Predicted\")\n",
    "ax.set_ylabel(\"Target\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5446d7e9-001e-4803-8de2-187ec4492e82",
   "metadata": {},
   "source": [
    "| Model              | Accuracy (%) | Strengths                                       | Weaknesses                                      |\n",
    "|--------------------|-------------|------------------------------------------------|------------------------------------------------|\n",
    "| KNN               | 74%          | Simple, good for pattern recognition           | Struggles with high-dimensional data, slow for large datasets |\n",
    "| Logistic Regression | 93%         | High interpretability, suitable for linear relationships | May not capture complex interactions |\n",
    "| Decision Tree     | 83%          | Handles non-linearity, interpretable           | Overfits easily, sensitive to small changes    |\n",
    "| Random Forest     | 94%          | High accuracy, reduces overfitting, robust    | Computationally expensive, less interpretable  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6ceb5479-c54a-4a0e-a40c-3cf1e713e5c8",
   "metadata": {},
   "source": [
    "### Conclusion on models :- Among the models, Random Forest demonstrated the best overall performance, making it the most suitable choice for heart disease prediction. However, Logistic Regression is also a viable option due to its interpretability and high accuracy. Decision Trees and KNN performed moderately but had limitations that impacted their effectiveness."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17d49873-fe41-43c5-bc22-9624a7371a14",
   "metadata": {},
   "source": [
    "# 4. Results and Discussion "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae5ccd73-71dc-4534-9240-3a1b75defb37",
   "metadata": {},
   "source": [
    "### After training and testing the four machine learning models, their performance was evaluated using accuracy, precision, recall, and F1-score. The following table summarizes the results:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b651db36-038e-43af-932c-1dcad1497b09",
   "metadata": {},
   "source": [
    "| Model                     | Accuracy (%) | Precision | Recall | F1-Score |\n",
    "|---------------------------|-------------|-----------|--------|----------|\n",
    "| K-Nearest Neighbors (KNN) | 74%         | 0.74      | 0.74   | 0.74     |\n",
    "| Logistic Regression       | 93%         | 0.93      | 0.92   | 0.93     |\n",
    "| Decision Tree             | 83%         | 0.83      | 0.83   | 0.83     |\n",
    "| Random Forest             | 94%         | 0.94      | 0.94   | 0.94     |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63b6986f-c06f-4ec7-ac60-8d1348109421",
   "metadata": {},
   "source": [
    "## 4.1 Model Performance Evaluation\n",
    "### K-Nearest Neighbors (KNN) – 74% Accuracy\n",
    "#### KNN had the lowest accuracy (74%), making it the least effective model for heart disease prediction. It works by measuring the Euclidean distance between data points and classifying them based on their nearest neighbors. However, KNN struggles with high-dimensional data, leading to increased misclassification rates.\n",
    "\n",
    "#### Precision & Recall: The 0.74 precision and recall scores indicate that KNN had many false positives and false negatives, reducing its reliability.\n",
    "#### F1-Score: The 0.74 F1-score confirms that the model performed poorly in balancing precision and recall.\n",
    "#### Overall, KNN is not suitable for this dataset due to its high computational cost, sensitivity to outliers, and weak performance.\n",
    "\n",
    "### Logistic Regression – 93% Accuracy\n",
    "#### Logistic Regression achieved 93% accuracy, making it one of the top-performing models. Since this dataset has a linear relationship between features and heart disease presence, Logistic Regression was able to classify patients effectively.\n",
    "\n",
    "#### Precision & Recall: The model had a high precision (0.93) and recall (0.92), indicating minimal false positives and false negatives.\n",
    "#### F1-Score: The 0.93 F1-score shows that the model was balanced in handling misclassifications.\n",
    "#### Logistic Regression is a strong candidate for medical applications, as it is interpretable, computationally efficient, and reliable.\n",
    "\n",
    "### Decision Tree Classifier – 83% Accuracy\n",
    "#### Decision Trees split the dataset into decision nodes to make classifications. This model achieved 83% accuracy, outperforming KNN but falling behind Logistic Regression and Random Forest.\n",
    "\n",
    "#### Precision & Recall: The 0.83 precision and recall scores indicate that the model performed well but was prone to overfitting.\n",
    "#### F1-Score: The 0.83 F1-score suggests that it performed adequately but struggled with generalization.\n",
    "#### Since Decision Trees can easily memorize training data, they often overfit, resulting in lower performance on unseen data.\n",
    "\n",
    "### Random Forest Classifier – 94% Accuracy\n",
    "#### Random Forest, an ensemble model that combines multiple Decision Trees, achieved the highest accuracy (94%). By using bootstrap aggregation (bagging), it reduced overfitting, making it the most reliable model.\n",
    "\n",
    "#### Precision & Recall: The 0.94 precision and recall scores indicate that Random Forest had the lowest misclassification rates.\n",
    "#### F1-Score: The 0.94 F1-score confirms that the model achieved the best balance between precision and recall.\n",
    "#### This model is highly effective for heart disease prediction, as it handles large datasets, non-linearity, and complex relationships between features.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8eb6c8d6-d538-48e9-9fc8-b25b2c3712ad",
   "metadata": {},
   "source": [
    "## 4.2 Confusion Matrix Analysis\n",
    "### The confusion matrix provides insights into the number of correctly and incorrectly classified cases. The Random Forest model had the lowest misclassification rate, confirming higher reliability.\n",
    "\n",
    "### Key Observations from Confusion Matrices:\n",
    "### KNN misclassified several heart disease cases, leading to high false positives and false negatives.\n",
    "### Logistic Regression and Random Forest had lower misclassification rates, making them more suitable for clinical applications.\n",
    "### Decision Trees performed moderately well but overfit the data, causing higher false positives than Random Forest."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f55357eb-ef91-4ef4-9c71-61b9bfa06196",
   "metadata": {},
   "source": [
    "| Model               | False Positives (Misdiagnosed as Heart Disease) | False Negatives (Missed Heart Disease Cases) |\n",
    "|---------------------|-----------------------------------------------|--------------------------------------------|\n",
    "| KNN                | High                                          | High                                      |\n",
    "| Logistic Regression | Low                                           | Low                                       |\n",
    "| Decision Tree      | Moderate                                      | Moderate                                  |\n",
    "| Random Forest      | Very Low                                      | Very Low                                  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5aa74e35-bf02-4bac-8191-e8f819d1b451",
   "metadata": {},
   "source": [
    "## 4.3 Discussion and Insights\n",
    "### The results suggest that Random Forest is the most reliable model for heart disease prediction. Logistic Regression is also an excellent alternative, especially in cases where interpretability is important.\n",
    "\n",
    "### KNN struggled with this dataset, as it is sensitive to high-dimensional features. Decision Trees, while effective, tend to overfit, reducing their generalizability.\n",
    "\n",
    "#### * Random Forest is the most robust model, as it reduces overfitting and provides the best balance between accuracy and generalization.\n",
    "#### * Logistic Regression is useful for real-world applications, as it is computationally efficient and interpretable.\n",
    "#### * Decision Trees perform well but require pruning to avoid overfitting.\n",
    "#### * KNN is not ideal for this dataset, as it lacks efficiency with large feature spaces."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11bcd6fc-ccf0-4b63-871c-7ac50c62664d",
   "metadata": {},
   "source": [
    "# 5.Conclusion"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0317d4c-5464-4de6-93c4-72abe19f6b1c",
   "metadata": {},
   "source": [
    "### This study demonstrated the effectiveness of machine learning models in predicting heart disease, showing that ML techniques can enhance early diagnosis, reduce misdiagnosis, and improve patient outcomes. By analyzing clinical features such as age, cholesterol levels, blood pressure, heart rate, and chest pain type, the models successfully classified patients based on their likelihood of having heart disease. Among the models tested, the Random Forest Classifier emerged as the most reliable, achieving 94% accuracy, followed closely by Logistic Regression at 93%. The Decision Tree Classifier performed moderately well at 83%, while the K-Nearest Neighbors (KNN) algorithm showed the lowest performance at 74% due to its sensitivity to high-dimensionality issues and overlapping feature distributions.\n",
    "\n",
    "### 5.1 Key Findings and Model Comparisons\n",
    "#### The results indicate that Random Forest outperformed all other models, making it the most suitable choice for heart disease prediction. The strength of Random Forest lies in its ability to reduce overfitting by aggregating multiple decision trees, resulting in higher accuracy, robustness, and generalizability. Logistic Regression also performed exceptionally well, demonstrating high interpretability and computational efficiency, making it a viable option for real-world applications in clinical settings.\n",
    "\n",
    "#### The Decision Tree classifier, while effective at identifying patterns within the data, was prone to overfitting, meaning it performed well on the training set but lacked generalizability on new data. On the other hand, KNN struggled with classifying heart disease cases due to the dataset’s complexity and high-dimensional features, making it less suitable for medical applications.\n",
    "\n",
    "#### The confusion matrix analysis provided further insights into model reliability and misclassification rates. The Random Forest model had the lowest number of false positives and false negatives, making it the most trustworthy model for heart disease prediction. KNN had the highest misclassification rate, reinforcing its unsuitability for this dataset. Logistic Regression and Decision Tree performed reasonably well, with Logistic Regression offering a better balance of interpretability and accuracy.\n",
    "\n",
    "### 5.2 The Role of Machine Learning in Cardiology\n",
    "#### Machine learning is revolutionizing the healthcare industry, particularly in predictive analytics and decision support systems. This study supports the growing body of evidence that ML-based models can play a crucial role in early risk assessment, improving diagnosis, and aiding clinical decision-making. By leveraging patient data, these models can identify high-risk individuals before symptoms manifest, allowing for timely medical intervention.\n",
    "\n",
    "#### The integration of machine learning into cardiology can address several challenges in traditional heart disease diagnostics, including:\n",
    "\n",
    "##### Reducing dependency on expensive diagnostic procedures, such as MRI scans, angiograms, and stress tests.\n",
    "##### Providing rapid, automated analysis of patient health records, eliminating human error.\n",
    "##### Enabling early detection and prevention, thereby reducing the global burden of cardiovascular diseases.\n",
    "#### Despite these advantages, ML models should not replace medical professionals but should be used as decision-support tools. A combination of ML-based predictions and expert analysis will lead to better patient outcomes and increased diagnostic efficiency.\n",
    "\n",
    "### 5.3 Future Enhancements and Research Directions\n",
    "#### While this study demonstrated the efficacy of machine learning models, several areas require further exploration to enhance accuracy, reliability, and real-world applicability:\n",
    "\n",
    "#### 1. Deep Learning Models\n",
    "#####  Neural networks and deep learning algorithms (e.g., Convolutional Neural Networks - CNNs and Recurrent Neural Networks - RNNs) could further improve predictive performance.\n",
    "##### Deep learning models can automatically extract hidden patterns from medical data, enhancing predictive accuracy.\n",
    "#### 2. Real-Time Patient Monitoring\n",
    "##### Integrating ML models with wearable technology and IoT devices (e.g., smartwatches, heart rate monitors, and ECG patches) could facilitate real-time heart disease prediction.\n",
    "#####  ML algorithms can analyze continuous health data and provide alerts for abnormal heart activity, enabling early intervention.\n",
    "#### 3. Integration with Electronic Health Records (EHRs)\n",
    "##### Combining ML models with electronic health records (EHRs) would allow for personalized heart disease risk assessments based on historical medical data.\n",
    "##### This integration can streamline medical decision-making and improve patient management.\n",
    "#### 4. Explainability and Interpretability of ML Models\n",
    "##### One of the challenges of ML in healthcare is the \"black-box\" nature of certain models (e.g., Random Forest and Deep Learning models).\n",
    "##### Research should focus on explainable AI (XAI) techniques, ensuring that ML models provide clear, interpretable results that clinicians can trust.\n",
    "#### 5. Addressing Class Imbalances in Medical Datasets\n",
    "##### Many medical datasets suffer from class imbalances, where non-disease cases significantly outnumber disease cases.\n",
    "##### Techniques such as Synthetic Minority Over-sampling Technique (SMOTE) or cost-sensitive learning should be explored to enhance model fairness and accuracy.\n",
    "#### 5.4 The Future of AI-Driven Healthcare\n",
    "##### The future of AI-driven healthcare is promising, with machine learning models evolving to become more precise, explainable, and integrated into clinical workflows. AI-powered diagnostic tools are being adopted worldwide to address healthcare challenges, such as:\n",
    "\n",
    "##### AI-assisted imaging analysis (e.g., detecting heart abnormalities from echocardiograms).\n",
    "##### Predictive analytics for chronic disease management (e.g., hypertension and diabetes risk prediction).\n",
    "##### AI-powered chatbots and virtual assistants to provide patient education and self-monitoring guidance.\n",
    "#### This study confirmed that machine learning offers a highly effective approach to predicting heart disease, with Random Forest emerging as the most reliable model (94% accuracy). Logistic Regression (93%) also proved to be a strong contender, particularly in scenarios requiring interpretability and efficiency. While Decision Trees (83%) showed promise, they were prone to overfitting, and KNN (74%) struggled with high-dimensional data.\n",
    "\n",
    "#### The findings highlight that ML models can enhance early detection, reduce healthcare costs, and improve patient survival rates. However, their full potential lies in integrating with real-world healthcare systems, such as EHRs, wearable health monitors, and AI-powered clinical decision support systems.\n",
    "\n",
    "#### Future work should focus on deep learning, real-time monitoring, and explainable AI techniques to make ML models more accurate, interpretable, and applicable to clinical settings. With continued advancements in AI and data-driven healthcare, ML models will play a crucial role in revolutionizing heart disease prediction and prevention, ultimately leading to better health outcomes for millions worldwide."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30ec09f3-881e-4902-92a6-57571b450007",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "1. Fedesoriano, Kaggle Dataset: Heart Failure Prediction. Available at: [https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction](https://www.kaggle.com/datasets/fedesoriano/heart-failure-prediction)\n",
    "2. World Health Organization (WHO) Report on Cardiovascular Diseases. Available at: [https://www.who.int/](https://www.who.int/)\n",
    "3. Scikit-Learn: Machine Learning in Python. Available at: [https://scikit-learn.org/](https://scikit-learn.org/)\n",
    "4. Pandas Development Team. (2023). pandas: Python Data Analysis Library. Available at: [https://pandas.pydata.org/](https://pandas.pydata.org/)\n",
    "5. NumPy Developers. (2023). NumPy: The Fundamental Package for Scientific Computing in Python. Available at: [https://numpy.org/](https://numpy.org/)\n",
    "6. Matplotlib Development Team. (2023). Matplotlib: Visualization with Python. Available at: [https://matplotlib.org/](https://matplotlib.org/)\n",
    "7. Seaborn Library. (2023). Seaborn: Statistical Data Visualization. Available at: [https://seaborn.pydata.org/](https://seaborn.pydata.org/)\n",
    "8. Pedregosa, F., Varoquaux, G., Gramfort, A., Michel, V., Thirion, B., Grisel, O., ... & Duchesnay, E. (2011). Scikit-learn: Machine Learning in Python. *Journal of Machine Learning Research, 12*, 2825-2830. Available at: [https://scikit-learn.org/](https://scikit-learn.org/)\n",
    "9. Han, J., Kamber, M., & Pei, J. (2011). *Data Mining: Concepts and Techniques.* Elsevier.\n",
    "10. Witten, I. H., Frank, E., & Hall, M. A. (2016). *Data Mining: Practical Machine Learning Tools and Techniques.* Morgan Kaufmann.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
